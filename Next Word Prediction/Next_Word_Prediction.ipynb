{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ddd15d97",
      "metadata": {
        "id": "ddd15d97"
      },
      "outputs": [],
      "source": [
        "#!pip install regex\n",
        "#!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "13fe6d2a",
      "metadata": {
        "id": "13fe6d2a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d8173856",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "d8173856",
        "outputId": "dd45c69b-9e98-43fc-fac1-044cf91a2697"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.net Title: The Adventures of Sherlock Holmes Author: Arthur Conan Doyle Release Date: November 29, 2002 [EBook #1661] Last Updated: May 20, 2019 Language: English Character set en\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "file = open('1661-0.txt', encoding=\"utf8\")\n",
        "\n",
        "#store file in a list\n",
        "lines = []\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "\n",
        "#Convert list to string\n",
        "data = \"\"\n",
        "for i in lines:\n",
        "    data = ' '.join(lines)\n",
        "    \n",
        "#Replace unnecessary stuff with space\n",
        "data = data.replace('\\n','').replace('\\r','').replace('\\ufeff','').replace('\"','')\n",
        "\n",
        "#Remove unnecessary space\n",
        "data = data.split()\n",
        "data = ' '.join(data)\n",
        "data[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5e24a3e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e24a3e9",
        "outputId": "9889367b-6c57-4384-a825-14ff3ce7e630"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "578728"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e4e38aa",
      "metadata": {
        "id": "2e4e38aa"
      },
      "source": [
        "## Apply Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7906cfe2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7906cfe2",
        "outputId": "dad833cb-da11-47d3-f510-360a43bdd926"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[145, 4789, 1, 1020, 4, 128, 34, 45, 611, 2235, 2236, 30, 1021, 15, 23]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "#Saving the tokenizer for predict function\n",
        "pickle.dump(tokenizer, open('token.pkl','wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2df6d2b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2df6d2b3",
        "outputId": "203cc68f-c9ab-464e-b1b6-bb30e48084d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111252"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "len(sequence_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "83e41795",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83e41795",
        "outputId": "09cffbc8-b3c9-4518-d122-342952ca4993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8931\n"
          ]
        }
      ],
      "source": [
        "vocabulary_size = len(tokenizer.word_index) + 1\n",
        "print(vocabulary_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "98edf861",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98edf861",
        "outputId": "2f9db8ba-b0d7-4b62-a01e-d3d3288a2b6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of sequences:  111249\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 145, 4789,    1, 1020],\n",
              "       [4789,    1, 1020,    4],\n",
              "       [   1, 1020,    4,  128],\n",
              "       [1020,    4,  128,   34],\n",
              "       [   4,  128,   34,   45],\n",
              "       [ 128,   34,   45,  611],\n",
              "       [  34,   45,  611, 2235],\n",
              "       [  45,  611, 2235, 2236],\n",
              "       [ 611, 2235, 2236,   30],\n",
              "       [2235, 2236,   30, 1021]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(3, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The length of sequences: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "75c9150b",
      "metadata": {
        "id": "75c9150b"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0:3])\n",
        "    y.append(i[3])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9da434a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9da434a2",
        "outputId": "4d9057da-f675-4f4f-c3d2-9a79b38ba40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data:  [[ 145 4789    1]\n",
            " [4789    1 1020]\n",
            " [   1 1020    4]\n",
            " [1020    4  128]\n",
            " [   4  128   34]\n",
            " [ 128   34   45]\n",
            " [  34   45  611]\n",
            " [  45  611 2235]\n",
            " [ 611 2235 2236]\n",
            " [2235 2236   30]]\n",
            "Response:  [1020    4  128   34   45  611 2235 2236   30 1021]\n"
          ]
        }
      ],
      "source": [
        "print(\"Data: \", X[:10])\n",
        "print(\"Response: \", y[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "eb7bcbe4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb7bcbe4",
        "outputId": "0f90410e-82ef-4e26-85ba-d82be2184689"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "y = to_categorical(y, num_classes=vocabulary_size)\n",
        "y[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f14d8d7b",
      "metadata": {
        "id": "f14d8d7b"
      },
      "source": [
        "## Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "b83a2dcc",
      "metadata": {
        "id": "b83a2dcc"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, 10, input_length=3))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocabulary_size, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "2a01963a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a01963a",
        "outputId": "06436fc7-0613-49d1-d489-cc5593a68ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 3, 10)             89310     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 3, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 8931)              8939931   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,078,241\n",
            "Trainable params: 22,078,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a8309a1",
      "metadata": {
        "id": "2a8309a1"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "aa82402f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa82402f",
        "outputId": "ce58ea84-27b5-4786-85da-355902a28c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.7637\n",
            "Epoch 1: loss improved from inf to 0.76373, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 36s 18ms/step - loss: 0.7637\n",
            "Epoch 2/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.6637\n",
            "Epoch 2: loss improved from 0.76373 to 0.66366, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.6637\n",
            "Epoch 3/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.6463\n",
            "Epoch 3: loss improved from 0.66366 to 0.64627, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.6463\n",
            "Epoch 4/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.6284\n",
            "Epoch 4: loss improved from 0.64627 to 0.62844, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.6284\n",
            "Epoch 5/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.6111\n",
            "Epoch 5: loss improved from 0.62844 to 0.61105, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.6111\n",
            "Epoch 6/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.5967\n",
            "Epoch 6: loss improved from 0.61105 to 0.59672, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.5967\n",
            "Epoch 7/70\n",
            "1736/1739 [============================>.] - ETA: 0s - loss: 0.5845\n",
            "Epoch 7: loss improved from 0.59672 to 0.58450, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.5845\n",
            "Epoch 8/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.5666\n",
            "Epoch 8: loss improved from 0.58450 to 0.56658, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.5666\n",
            "Epoch 9/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.5607\n",
            "Epoch 9: loss improved from 0.56658 to 0.56071, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.5607\n",
            "Epoch 10/70\n",
            "1736/1739 [============================>.] - ETA: 0s - loss: 0.5512\n",
            "Epoch 10: loss improved from 0.56071 to 0.55153, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 29s 16ms/step - loss: 0.5515\n",
            "Epoch 11/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.5428\n",
            "Epoch 11: loss improved from 0.55153 to 0.54276, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.5428\n",
            "Epoch 12/70\n",
            "1737/1739 [============================>.] - ETA: 0s - loss: 0.5316\n",
            "Epoch 12: loss improved from 0.54276 to 0.53165, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.5316\n",
            "Epoch 13/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.5229\n",
            "Epoch 13: loss improved from 0.53165 to 0.52291, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.5229\n",
            "Epoch 14/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.5161\n",
            "Epoch 14: loss improved from 0.52291 to 0.51620, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 29s 17ms/step - loss: 0.5162\n",
            "Epoch 15/70\n",
            "1737/1739 [============================>.] - ETA: 0s - loss: 0.5065\n",
            "Epoch 15: loss improved from 0.51620 to 0.50669, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.5067\n",
            "Epoch 16/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.5020\n",
            "Epoch 16: loss improved from 0.50669 to 0.50205, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 29s 16ms/step - loss: 0.5020\n",
            "Epoch 17/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.4952\n",
            "Epoch 17: loss improved from 0.50205 to 0.49517, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 29s 16ms/step - loss: 0.4952\n",
            "Epoch 18/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.4887\n",
            "Epoch 18: loss improved from 0.49517 to 0.48876, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4888\n",
            "Epoch 19/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.4865\n",
            "Epoch 19: loss improved from 0.48876 to 0.48647, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4865\n",
            "Epoch 20/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.4779\n",
            "Epoch 20: loss improved from 0.48647 to 0.47794, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4779\n",
            "Epoch 21/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.4720\n",
            "Epoch 21: loss improved from 0.47794 to 0.47206, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4721\n",
            "Epoch 22/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.4714\n",
            "Epoch 22: loss improved from 0.47206 to 0.47145, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4714\n",
            "Epoch 23/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.4672\n",
            "Epoch 23: loss improved from 0.47145 to 0.46716, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 30s 17ms/step - loss: 0.4672\n",
            "Epoch 24/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.4610\n",
            "Epoch 24: loss improved from 0.46716 to 0.46105, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4610\n",
            "Epoch 25/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.4514\n",
            "Epoch 25: loss improved from 0.46105 to 0.45137, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4514\n",
            "Epoch 26/70\n",
            "1737/1739 [============================>.] - ETA: 0s - loss: 0.4516\n",
            "Epoch 26: loss did not improve from 0.45137\n",
            "1739/1739 [==============================] - 27s 15ms/step - loss: 0.4516\n",
            "Epoch 27/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.4525\n",
            "Epoch 27: loss did not improve from 0.45137\n",
            "1739/1739 [==============================] - 27s 16ms/step - loss: 0.4525\n",
            "Epoch 28/70\n",
            "1737/1739 [============================>.] - ETA: 0s - loss: 0.4464\n",
            "Epoch 28: loss improved from 0.45137 to 0.44646, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4465\n",
            "Epoch 29/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.4427\n",
            "Epoch 29: loss improved from 0.44646 to 0.44268, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4427\n",
            "Epoch 30/70\n",
            "1737/1739 [============================>.] - ETA: 0s - loss: 0.4373\n",
            "Epoch 30: loss improved from 0.44268 to 0.43725, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 30s 17ms/step - loss: 0.4372\n",
            "Epoch 31/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.4323\n",
            "Epoch 31: loss improved from 0.43725 to 0.43235, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4323\n",
            "Epoch 32/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.4359\n",
            "Epoch 32: loss did not improve from 0.43235\n",
            "1739/1739 [==============================] - 27s 15ms/step - loss: 0.4359\n",
            "Epoch 33/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.4306\n",
            "Epoch 33: loss improved from 0.43235 to 0.43064, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4306\n",
            "Epoch 34/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.4265\n",
            "Epoch 34: loss improved from 0.43064 to 0.42652, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4265\n",
            "Epoch 35/70\n",
            "1737/1739 [============================>.] - ETA: 0s - loss: 0.4261\n",
            "Epoch 35: loss improved from 0.42652 to 0.42612, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4261\n",
            "Epoch 36/70\n",
            "1737/1739 [============================>.] - ETA: 0s - loss: 0.4237\n",
            "Epoch 36: loss improved from 0.42612 to 0.42372, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4237\n",
            "Epoch 37/70\n",
            "1736/1739 [============================>.] - ETA: 0s - loss: 0.4179\n",
            "Epoch 37: loss improved from 0.42372 to 0.41808, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4181\n",
            "Epoch 38/70\n",
            "1737/1739 [============================>.] - ETA: 0s - loss: 0.4148\n",
            "Epoch 38: loss improved from 0.41808 to 0.41490, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4149\n",
            "Epoch 39/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.4139\n",
            "Epoch 39: loss improved from 0.41490 to 0.41393, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4139\n",
            "Epoch 40/70\n",
            "1736/1739 [============================>.] - ETA: 0s - loss: 0.4108\n",
            "Epoch 40: loss improved from 0.41393 to 0.41096, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4110\n",
            "Epoch 41/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.4105\n",
            "Epoch 41: loss improved from 0.41096 to 0.41047, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 29s 17ms/step - loss: 0.4105\n",
            "Epoch 42/70\n",
            "1737/1739 [============================>.] - ETA: 0s - loss: 0.4087\n",
            "Epoch 42: loss improved from 0.41047 to 0.40865, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4086\n",
            "Epoch 43/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.4067\n",
            "Epoch 43: loss improved from 0.40865 to 0.40674, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4067\n",
            "Epoch 44/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.4041\n",
            "Epoch 44: loss improved from 0.40674 to 0.40407, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 29s 16ms/step - loss: 0.4041\n",
            "Epoch 45/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.4048\n",
            "Epoch 45: loss did not improve from 0.40407\n",
            "1739/1739 [==============================] - 27s 15ms/step - loss: 0.4048\n",
            "Epoch 46/70\n",
            "1736/1739 [============================>.] - ETA: 0s - loss: 0.4015\n",
            "Epoch 46: loss improved from 0.40407 to 0.40177, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4018\n",
            "Epoch 47/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.4004\n",
            "Epoch 47: loss improved from 0.40177 to 0.40035, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.4004\n",
            "Epoch 48/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.3963\n",
            "Epoch 48: loss improved from 0.40035 to 0.39631, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.3963\n",
            "Epoch 49/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.3988\n",
            "Epoch 49: loss did not improve from 0.39631\n",
            "1739/1739 [==============================] - 27s 15ms/step - loss: 0.3988\n",
            "Epoch 50/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.3936\n",
            "Epoch 50: loss improved from 0.39631 to 0.39353, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.3935\n",
            "Epoch 51/70\n",
            "1736/1739 [============================>.] - ETA: 0s - loss: 0.3904\n",
            "Epoch 51: loss improved from 0.39353 to 0.39048, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.3905\n",
            "Epoch 52/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.3917\n",
            "Epoch 52: loss did not improve from 0.39048\n",
            "1739/1739 [==============================] - 27s 15ms/step - loss: 0.3917\n",
            "Epoch 53/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.3891\n",
            "Epoch 53: loss improved from 0.39048 to 0.38913, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.3891\n",
            "Epoch 54/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.3865\n",
            "Epoch 54: loss improved from 0.38913 to 0.38655, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.3866\n",
            "Epoch 55/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.3862\n",
            "Epoch 55: loss improved from 0.38655 to 0.38618, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.3862\n",
            "Epoch 56/70\n",
            "1737/1739 [============================>.] - ETA: 0s - loss: 0.3924\n",
            "Epoch 56: loss did not improve from 0.38618\n",
            "1739/1739 [==============================] - 27s 15ms/step - loss: 0.3925\n",
            "Epoch 57/70\n",
            "1736/1739 [============================>.] - ETA: 0s - loss: 0.3821\n",
            "Epoch 57: loss improved from 0.38618 to 0.38230, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.3823\n",
            "Epoch 58/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.3794\n",
            "Epoch 58: loss improved from 0.38230 to 0.37944, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.3794\n",
            "Epoch 59/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.3827\n",
            "Epoch 59: loss did not improve from 0.37944\n",
            "1739/1739 [==============================] - 27s 16ms/step - loss: 0.3827\n",
            "Epoch 60/70\n",
            "1736/1739 [============================>.] - ETA: 0s - loss: 0.3830\n",
            "Epoch 60: loss did not improve from 0.37944\n",
            "1739/1739 [==============================] - 27s 15ms/step - loss: 0.3832\n",
            "Epoch 61/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.3770\n",
            "Epoch 61: loss improved from 0.37944 to 0.37698, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.3770\n",
            "Epoch 62/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.3788\n",
            "Epoch 62: loss did not improve from 0.37698\n",
            "1739/1739 [==============================] - 27s 16ms/step - loss: 0.3788\n",
            "Epoch 63/70\n",
            "1736/1739 [============================>.] - ETA: 0s - loss: 0.3800\n",
            "Epoch 63: loss did not improve from 0.37698\n",
            "1739/1739 [==============================] - 27s 15ms/step - loss: 0.3800\n",
            "Epoch 64/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.3734\n",
            "Epoch 64: loss improved from 0.37698 to 0.37338, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.3734\n",
            "Epoch 65/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.3728\n",
            "Epoch 65: loss improved from 0.37338 to 0.37283, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.3728\n",
            "Epoch 66/70\n",
            "1739/1739 [==============================] - ETA: 0s - loss: 0.3732\n",
            "Epoch 66: loss did not improve from 0.37283\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.3732\n",
            "Epoch 67/70\n",
            "1736/1739 [============================>.] - ETA: 0s - loss: 0.3745\n",
            "Epoch 67: loss did not improve from 0.37283\n",
            "1739/1739 [==============================] - 27s 16ms/step - loss: 0.3747\n",
            "Epoch 68/70\n",
            "1736/1739 [============================>.] - ETA: 0s - loss: 0.3740\n",
            "Epoch 68: loss did not improve from 0.37283\n",
            "1739/1739 [==============================] - 27s 15ms/step - loss: 0.3741\n",
            "Epoch 69/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.3708\n",
            "Epoch 69: loss improved from 0.37283 to 0.37082, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 28s 16ms/step - loss: 0.3708\n",
            "Epoch 70/70\n",
            "1738/1739 [============================>.] - ETA: 0s - loss: 0.3706\n",
            "Epoch 70: loss improved from 0.37082 to 0.37061, saving model to next_words.h5\n",
            "1739/1739 [==============================] - 29s 16ms/step - loss: 0.3706\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e69055d00>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor=\"loss\", verbose=1, save_best_only=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
        "model.fit(X, y, epochs=70, batch_size=64, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prediction**"
      ],
      "metadata": {
        "id": "o7kDA9z9tpGv"
      },
      "id": "o7kDA9z9tpGv"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "1e719d76",
      "metadata": {
        "id": "1e719d76"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "#Load the model and tokenizer\n",
        "model = load_model(\"next_words.h5\")\n",
        "tokenizer = pickle.load(open(\"token.pkl\", \"rb\"))\n",
        "\n",
        "def predict_next_word(model, tokenizer, text):\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence))\n",
        "  predicted_word = \" \"\n",
        "\n",
        "  for key, value in tokenizer.word_index.items():\n",
        "    if value == preds:\n",
        "      predicted_word = key\n",
        "      break\n",
        "  print(predicted_word)\n",
        "  return predicted_word"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "  text = input(\"Enter the text: \")\n",
        "\n",
        "  if text == \"0\":\n",
        "    print(\"Execution Completed\")\n",
        "    break\n",
        "  \n",
        "  else:\n",
        "    try:\n",
        "      text = text.split(\" \")\n",
        "      text = text[-3:]\n",
        "      print(text)\n",
        "\n",
        "      predict_next_word(model, tokenizer, text)\n",
        "    \n",
        "    except Exception as e:\n",
        "      print(\"Error Occured: \",e)\n",
        "      continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBqIir7IuuND",
        "outputId": "cc320aca-e62a-4db1-f52f-aa41a3fb1999"
      },
      "id": "jBqIir7IuuND",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text: of Sherlock Holmes\n",
            "['of', 'Sherlock', 'Holmes']\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "by\n",
            "Enter the text: single man of\n",
            "['single', 'man', 'of']\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "the\n",
            "Enter the text: Sir Williams and \n",
            "['Williams', 'and', '']\n",
            "1/1 [==============================] - 1s 716ms/step\n",
            "then\n",
            "Enter the text: the lady is\n",
            "['the', 'lady', 'is']\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "correct\n",
            "Enter the text: the student is\n",
            "['the', 'student', 'is']\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "one\n",
            "Enter the text: 0\n",
            "Execution Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1q50nXjwuzb_"
      },
      "id": "1q50nXjwuzb_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}